CEU MicroData
=============
Data Backend Call for Bids
--------------------------

We want to build a data backend for a web application serving economic time series data. Our backend will serve the following functions:

1. Load data into memory or an on-disk staging area from various sources:
 - SQL database (Postgres, MySQL, sqlite)
 - separated, fixed-width and JSON text file located on
 	- local server
 	- http
 	- git repository
 - Google spreadsheet
Data sources will be defined in a config file, in any human- and machine-readable format (.ini config file, YAML, JSON or XML, any simple scripting language). Beyond checking for resource availability (missing file, 404 on http), no further data auditing is needed at this point. We currently have about 30 different data sets.

Data sets contain either tabular data (where each row has the same columns), or documents. We do not currently store relational or hierarchical data in document stores, so each document is a text or an associative array. Data sets have a fixed schema which can be defined in the config file.

The size of data sets varies widely. We have some SQL tables with millions of rows and many flat files with rows in the low tens of thousands. We currently store some data sets chunked into separate files, e.g., we have fixed-width flat files in "election/parliamentary/1990/lis_szkt.txt" and "election/parliamentary/1994/lis_szkt.txt". If possible, we would like to refer to these as one dataset, indexing the time dimension transparently.

We would prefer a workflow linking to the original data sources, but we are also interested in an ETL solution, where a script first loads these data sets into a unified data store (an RDBMS or a document store), and the webserver is running queries against this data store. In the latter case, the need for caching (component 5) depends on performance.

2. Maintain a unified namespace for data sets, mapping URIs to actual sources, e.g.
"/geo/whitelist/street/election" would point to a Git repository holding .csv files of street names in the Hungarian electoral districts dataset. Each resource loaded in step 1 should have a unique URI. URIs should follow an unambiguous tree structure.

3. Design a RESTful API that serves time series data. E.g., "GET /macro/gdp/hungary" should yield a JSON document of all available Hungarian GDP data. Here "/macro/gdp" refers to the data set, "/hungary" selects the country of Hungary. (Also see filtering options belo.)

Only GET requests are relevant for now. This application will work on a single dataset with a unified data structure (e.g., /macro/gdp/poland, /macro/unemployment/hungary), not on all the datasets we have. The actual data may come from different sources, however, and it should be transparent to the user (see points 1 and 2).

A partial solution may expose only some of the above resource types to the API. In the first project, we only need to present macroeconomic time series, but we are also interested in a more general data API. The mappings and access control should be set in a config file. Access is regulated at the level of data set.

4. (optional) Allow filtering of time-series data through URL and GET parameters, e.g. "GET /macro/gdp/hungary?start=1992-Q1&end=2010-Q4". The filter should handle the time dimension only, selecting start and end dates. Additional filtering possibilities are welcome, but not required.

5. Cache the most requested time series data so that GET requests do not query the underlying source data. Set simple expiry for the cache, e.g. "/macro/gdp expires in 24 hours".

6. (optional) Propose solutions for a unified user interface to enter and update data. Given a structure defined in the config file, expose some of the fields for easy editing. E.g., add new quarterly GDP numbers. Metadata will not be edited by users, they will be edited by administrators in config files.

You will select the components (database engine, webserver, web framework, cache, ETL scripts), customize them and write necessary custom modules to build our backend. You will develop, test and deploy the backend on our production server. If you are only interested in parts of the project, please note so in your bid.

In your bid, please provide a price quote separately for each of the functionalities as follows: estimated number of work hours, hourly rate, estimated time of completion (in work days). Please refer to the original numbering 1 through 6. Also include an hourly rate for overtime and for after-sales support and consulting. Please name, without commitment, at least one tool for each component that you think would be suitable. We prefer open source tools, but other industry-standard tools are also acceptable. In this case, please provide specific licensing costs. 

Please submit your bid in English, addressed to 

	Miklós Koren
	associate professor
	Közép-európai Egyetem
	Nádor u. 9.
	1051 Budapest.

Feel free to include any relevant references.

Your price quote will help us select the best solution for our needs. If necessary, we may mix and match suppliers for the different components. Based on subsequent negotiations, we would like to offer a time contract, paying an hourly rate with a specified minimum and maximum total amount. 

The purchase is financed by the European Research Council (ERC KNOWLEDGEFOWS) and is subject to university procurement and ethical conduct policies.
